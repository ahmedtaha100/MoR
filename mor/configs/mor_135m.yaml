model:
  hidden_dim: 576
  num_attention_heads: 9
  num_kv_heads: 3
  head_dim: 64
  num_shared_layers: 10
  num_recursion_steps: 3
  ffn_dim: 1536
  ffn_hidden_act: "silu"
  vocab_size: 49152
  max_seq_len: 2048
  router_type: "expert_choice"
  capacity_ratio: 0.5
  router_aux_loss_coeff: 0.001
  router_z_loss_coeff: 0.001
  router_activation: "sigmoid"
  router_architecture: "linear"
  router_alpha: 1.0
  capacity_warmup_steps: 1000
  kv_cache_strategy: "selective"
  sharing_strategy: "middle_cycle"
  dropout: 0.0
  attention_dropout: 0.0
  tie_word_embeddings: true
  rms_norm_eps: 1e-6
  initializer_range: 0.02
  gradient_checkpointing: false
  rope_theta: 10000.0

training:
  learning_rate: 3e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  warmup_steps: 2000
  max_steps: 100000
  min_lr_ratio: 0.1
  lr_scheduler: "trapezoid"
  warmup_ratio: 0.02
  cooldown_ratio: 0.2
  batch_size: 32
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  mixed_precision: "bf16"
